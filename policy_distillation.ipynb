{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "from baselines.common.atari_wrappers import make_atari, wrap_deepmind\n",
    "from baselines import bench\n",
    "\n",
    "from utils.gym_wrapper import WrapPyTorch\n",
    "from utils.agent import agent\n",
    "from utils.config import *\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from PIL import Image\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulate Teacher Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hamid/anaconda3/envs/doom-rl/lib/python3.6/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    }
   ],
   "source": [
    "env_id = \"BreakoutNoFrameskip-v4\"\n",
    "env = make_atari(env_id)\n",
    "env = bench.Monitor(env, os.path.join(\"log\", env_id))\n",
    "env = wrap_deepmind(env, episode_life=True, clip_rewards=True, frame_stack=False, scale=True)\n",
    "env = WrapPyTorch(env)\n",
    "teacher = agent(env=env)\n",
    "teacher.load_w()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_TEACHER_SIMULATIONS = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_observation = []\n",
    "list_of_states = []\n",
    "list_of_q_values = []\n",
    "observation = env.reset()\n",
    "\n",
    "for frame_idx in range(1, MAX_TEACHER_SIMULATIONS + 1):\n",
    "    teacher_observation.append(observation)\n",
    "    X = torch.tensor([observation], device=device, dtype=torch.float)\n",
    "    Q = teacher.model(X).data\n",
    "    list_of_states.append(X)\n",
    "    list_of_q_values.append(Q)\n",
    "    action = teacher.get_action(observation, 0.01)\n",
    "\n",
    "    prev_observation=observation\n",
    "    observation, _, done, _ = env.step(action)\n",
    "    observation = None if done else observation\n",
    "\n",
    "    if done:\n",
    "        observation = env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Teacher Simulation For Create Videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save teacher simulation in TEACHER FOLDER\n",
    "for index,obs in enumerate(teacher_observation):\n",
    "    #Rescale to 0-255 and convert to uint8\n",
    "    data = obs[0]\n",
    "    rescaled = (255.0 / data.max() * (data - data.min())).astype(np.uint8)\n",
    "\n",
    "    im = Image.fromarray(rescaled)\n",
    "    im.save('TEACHER/%s.png'%str(index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Tensor From states and q_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2.7094, 2.7138, 2.7128, 2.7907],\n",
       "        [2.7149, 2.7421, 2.7498, 2.7578],\n",
       "        [2.6924, 2.7157, 2.7606, 2.7533],\n",
       "        ...,\n",
       "        [2.4868, 2.4219, 2.4703, 2.4487],\n",
       "        [2.4538, 2.3915, 2.4308, 2.4219],\n",
       "        [2.4762, 2.4093, 2.4672, 2.4565]], device='cuda:0')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y = torch.Tensor(MAX_TEACHER_SIMULATIONS,1,4).cuda()\n",
    "torch.cat(list_of_q_values, out=Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
       "\n",
       "\n",
       "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
       "\n",
       "\n",
       "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
       "\n",
       "\n",
       "        ...,\n",
       "\n",
       "\n",
       "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
       "\n",
       "\n",
       "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
       "\n",
       "\n",
       "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = torch.Tensor(MAX_TEACHER_SIMULATIONS, 1,1,84,84).cuda()\n",
    "torch.cat(list_of_states, out=inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.squeeze(inputs,1).view(MAX_TEACHER_SIMULATIONS,84*84)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5000, 7056])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5000, 4])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create MSE Student"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MSE_Net(torch.nn.Module):\n",
    "    def __init__(self, n_feature, n_hidden, n_output):\n",
    "        super(MSE_Net, self).__init__()\n",
    "        self.hidden = torch.nn.Linear(n_feature, n_hidden)   # hidden layer\n",
    "        self.predict = torch.nn.Linear(n_hidden, n_output)   # output layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.hidden(x))      # activation function for hidden layer\n",
    "        x = self.predict(x)             # linear output\n",
    "        return x\n",
    "    \n",
    "class MSE_Student():\n",
    "    def __init__(self, net, device):\n",
    "        self.net = net\n",
    "        self.device=device\n",
    "        self.net = self.net.to(device)\n",
    "        self.optimizer = torch.optim.SGD(net.parameters(), lr=0.02)\n",
    "        self.loss_func = torch.nn.MSELoss()\n",
    "        \n",
    "    def train(self,x,Y,epoch=1000):\n",
    "        for t in range(epoch):\n",
    "            prediction = self.net(x)     # input x and predict based on x\n",
    "\n",
    "            loss = self.loss_func(prediction, Y)     # must be (1. nn output, 2. target)\n",
    "\n",
    "            self.optimizer.zero_grad()   # clear gradients for next train\n",
    "            loss.backward(retain_graph=True)    # backpropagation, compute gradients\n",
    "            self.optimizer.step()  # apply gradients\n",
    "            \n",
    "    def get_action(self,s,num_actions, eps=0.1):\n",
    "        with torch.no_grad():\n",
    "            if np.random.random() >= eps:\n",
    "                x_obs = torch.tensor([s], device=device, dtype=torch.float).view(-1,84*84)\n",
    "                a = self.net(x_obs).max(1)[1].view(1, 1)\n",
    "                return a.item()\n",
    "            else:\n",
    "                return np.random.randint(0, num_actions)\n",
    "            \n",
    "    def save_video(self,env,max_frames):\n",
    "        observation = env.reset()\n",
    "        for frame_idx in range(1, max_frames + 1):\n",
    "            data_for_saved = observation[0]\n",
    "            im = Image.fromarray((255.0 / data_for_saved.max() * (data_for_saved - data_for_saved.min())).astype(np.uint8))\n",
    "            \n",
    "            im.save('STUDENT_MSE/%s.png'%str(frame_idx))\n",
    "            action = self.get_action(observation,env.action_space.n)\n",
    "\n",
    "            observation, reward, done, _ = env.step(action)\n",
    "            observation = None if done else observation\n",
    "\n",
    "            if done:\n",
    "                observation = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_net = MSE_Net(n_feature=84*84, n_hidden=500, n_output=4)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_student = MSE_Student(mse_net,device)\n",
    "mse_student.train(x,Y,epoch=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_student.save_video(env,max_frames=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2.0373, 2.0245, 2.0213, 2.0390],\n",
       "        [2.0359, 2.0282, 2.0194, 2.0394],\n",
       "        [2.0316, 2.0233, 2.0099, 2.0295],\n",
       "        ...,\n",
       "        [2.0322, 2.0286, 2.0131, 2.0355],\n",
       "        [2.0306, 2.0241, 2.0174, 2.0337],\n",
       "        [2.0280, 2.0246, 2.0124, 2.0351]], device='cuda:0',\n",
       "       grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mse_net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Distilled_KL Student"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Distilled_KL_Net(torch.nn.Module):\n",
    "    def __init__(self, n_feature, n_hidden, n_output):\n",
    "        super(Distilled_KL_Net, self).__init__()\n",
    "        self.hidden = torch.nn.Linear(n_feature, n_hidden)   # hidden layer\n",
    "        self.predict = torch.nn.Linear(n_hidden, n_output)   # output layer\n",
    "\n",
    "    def forward(self, x, eps=0.00001):\n",
    "        x = F.relu(self.hidden(x))      # activation function for hidden layer\n",
    "        x = self.predict(x)             # linear output\n",
    "        x = F.softmax(x)+eps\n",
    "        return x\n",
    "    \n",
    "class Distilled_KL_Student():\n",
    "    def __init__(self, net, device):\n",
    "        self.net = net\n",
    "        self.device=device\n",
    "        self.net = self.net.to(device)\n",
    "        self.optimizer = torch.optim.SGD(net.parameters(), lr=0.02)\n",
    "        self.loss_func = torch.nn.KLDivLoss()\n",
    "        \n",
    "    def train(self,x,Y,epoch=1000):\n",
    "        for t in range(epoch):\n",
    "            prediction = self.net(x)     # input x and predict based on x\n",
    "\n",
    "            loss = self.loss_func(prediction, Y)     # must be (1. nn output, 2. target)\n",
    "\n",
    "            self.optimizer.zero_grad()   # clear gradients for next train\n",
    "            loss.backward(retain_graph=True)    # backpropagation, compute gradients\n",
    "            self.optimizer.step()  # apply gradients\n",
    "            \n",
    "    def get_action(self,s,num_actions, eps=0.1):\n",
    "        with torch.no_grad():\n",
    "            if np.random.random() >= eps:\n",
    "                x_obs = torch.tensor([s], device=device, dtype=torch.float).view(-1,84*84)\n",
    "                a = self.net(x_obs).max(1)[1].view(1, 1)\n",
    "                return a.item()\n",
    "            else:\n",
    "                return np.random.randint(0, num_actions)\n",
    "            \n",
    "    def save_video(self,env,max_frames):\n",
    "        observation = env.reset()\n",
    "        for frame_idx in range(1, max_frames + 1):\n",
    "            data_for_saved = observation[0]\n",
    "            im = Image.fromarray((255.0 / data_for_saved.max() * (data_for_saved - data_for_saved.min())).astype(np.uint8))\n",
    "            \n",
    "            im.save('STUDENT_DISTILLED_KL/%s.png'%str(frame_idx))\n",
    "            action = self.get_action(observation,env.action_space.n)\n",
    "\n",
    "            observation, reward, done, _ = env.step(action)\n",
    "            observation = None if done else observation\n",
    "\n",
    "            if done:\n",
    "                observation = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "distilled_kl_net = Distilled_KL_Net(n_feature=84*84, n_hidden=500, n_output=4)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hamid/anaconda3/envs/doom-rl/lib/python3.6/site-packages/ipykernel_launcher.py:3: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "tau = 0.01\n",
    "eps = 0.00001\n",
    "Y_kl = F.softmax(Y/tau)+eps "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hamid/anaconda3/envs/doom-rl/lib/python3.6/site-packages/ipykernel_launcher.py:10: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    }
   ],
   "source": [
    "distilled_kl_student = Distilled_KL_Student(distilled_kl_net,device)\n",
    "distilled_kl_student.train(x,Y_kl,epoch=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hamid/anaconda3/envs/doom-rl/lib/python3.6/site-packages/ipykernel_launcher.py:10: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    }
   ],
   "source": [
    "distilled_kl_student.save_video(env,max_frames=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hamid/anaconda3/envs/doom-rl/lib/python3.6/site-packages/ipykernel_launcher.py:10: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.1188, 0.0867, 0.3074, 0.4871],\n",
       "        [0.1191, 0.0868, 0.3060, 0.4882],\n",
       "        [0.1200, 0.0872, 0.3056, 0.4873],\n",
       "        ...,\n",
       "        [0.1254, 0.0877, 0.3026, 0.4843],\n",
       "        [0.1250, 0.0876, 0.3022, 0.4853],\n",
       "        [0.1256, 0.0878, 0.3021, 0.4845]], device='cuda:0',\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distilled_kl_net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python doom-rl",
   "language": "python",
   "name": "doom-rl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
